---
title: "Generalised Linear Models"
author: 
- Yasin Gija
date: "2024-04-21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question (a)

We will use a generalised linear model with the response Y which represents the advance rate of the drill. The response will follow a Gamma distribution $$Y_i \sim Gamma(\mu_i,\phi)$$ The linear predictor is $$\eta=\beta_0 +\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4$$ where $\beta_0$ is the intercept and $\beta_1,\beta_2,\beta_3,\beta_4$ are the coefficients for $X_1,X_2,X_3,X_4$ respectively.

The canonical link function for the above Gamma distribution for the response is $$g(\mu)=-1/\mu$$

# Question (b)

```{r pressure, echo=TRUE}
# First, read in the data to be used for the model
drill <- read.csv("drill.csv")

# Convert "Low" and "High" into levels 0 and 1
drill$X1 <- factor(drill$X1, levels = c("Low","High"))
drill$X2 <- factor(drill$X2, levels = c("Low","High"))
drill$X3 <- factor(drill$X3, levels = c("Low","High"))
drill$X4 <- factor(drill$X4, levels = c("Low","High"))

# Fit our generalised linear model
fitted_model <- glm(y ~ X1 + X2 + X3 + X4, family=Gamma(link="inverse"),data = drill)

# Display a summary of the fitted model
summary(fitted_model)
```

The signs of the parameter estimates $X_1,X_2,X_3,X_4$ are all negative, as shown in the 'Estimate' column. Having negative estimates is necessary as the R link corresponds to multiplying the coefficients $\beta_i$ by $-1$, giving us positive values. We need postive values as the Gamma distribution is only defined for positive values.

# Question (c)

```{r qu_c, echo=TRUE}
# Fit our second glm
fitted_model_2 <- glm(y ~ X1 + X2 + X3 + X4, family=Gamma(link="log"),data = drill)

# Display a summary
summary(fitted_model_2)
```

For this fitted model, we can see that the signs of the parameter estimates are all positive unlike our model from part (b). This means that we do not have negative $\eta$ values and so the expectation is not negative also.

# Question (d)

We can see that the model in part (c) has lower p values than in model (b). Smaller p values show us that the regression coefficient is non-zero and that the predictors with small p-values need to be included in the model.

Another observation we can make is that the residual deviance is 0.50836 on 11 degrees of freedom for the model in part (b) but is 0.10527 on 11 degrees of freedom in the model in part (c). This residual deviance is lower in part (c). A lower residual deviance implies that the model fits the data better.

One last aspect we can observe is that the AIC in the model in part (b) is 53.432 but in the model in part (c) it is 28.17, which is lower. A lower AIC score indicates that the model is a better fit.

# Question (e)

Two alternative ways we can use for testing the significance of a parameter are the Wald test and the Profile likelihood interval.

For both, we conduct a hypothesis test $$H_0: \beta_j = c \quad \text{vs} \quad H_1: \beta_j \neq c$$ where c is some value which is known.

\underline{Wald test}

We calculate the Wald interval using $$(\hat{\beta}_j - z_{\alpha/2}\text{s.e.}(\hat{\beta}_j), \hat{\beta}_j + z_{\alpha/2} \text{s.e.}(\hat{\beta}_j))$$

where $\hat{\beta}_j$ is the parameter estimate (obtained from the summary of the model in part (b)), $z_{\alpha/2}$ is the critical value obtained from the standard normal distribution for a two-tailed test for significance level $\alpha$ and $\text{s.e.}(\hat{\beta}_j)$ is the standard error (obtained from the summary of the model in part (b) also).

Putting this equation into R we get the confidence interval stated below for $X_1$

```{r qu_e, echo=TRUE}
-0.01658 + c(-1,1)*qnorm(0.975)*0.01434
```

We reject $H_0$ if our value does not lie within the confidence interval (-0.04468588,0.01152588). For $c=0$ we can see that $c$ does lie within our confidence interval, so $H_0$ is not rejected.

\underline{Profile Likelihood Interval}

We calculate the Profile likelihood interval using $$2 \left\{ \ell(\hat{\beta}_j) - \ell(\beta_j) \right\} \leq \chi^2_{\alpha,1}$$

where $\chi^2_{\alpha,1}$ is the critical value obtained from the chi-squared distribution for confidence level $1-\alpha$ with one degree of freedom. In R we can calculate this using the confint() command.

```{r qu_e2, echo=TRUE}
confint(fitted_model)
```

The confidence interval we get for $X_1$ is (-0.0453341,0.01124852). $c=0$ lies within this confidence interval so $H_0$ is not rejected.

# Question (f)

The two confidence intervals are very similar in size, with the profile likelihood CI being slightly larger than the Wald CI. However, the profile likelihood CI is better as it can capture the asymmetry in the likelihood function and so it is symmetric around the MLE. On the other hand, even though Wald is simpler, Wald cannot capture the asymmetry as good as the profile likelihood does.

# Question (g)

```{r qu_g, echo=TRUE}
# Fit our extended glm which includes two-factor interactions.
extended_model_2 <- update(fitted_model_2, ~.+ X1:X2 + X1:X3 + X1:X4 + X2:X3 + X2:X4 + X3:X4)

# Display a summary
summary(extended_model_2)
```

```{r qu_g2, echo=TRUE}
# Compare the model with model in part (c) using an F test.
anova(fitted_model_2,extended_model_2,test="F")
```

The deviance in model (g) is lower than the deviance in model (c). A lower residual deviance indicates that the model fits the data better. So, in terms of the deviance, extended_model_2 ,from part (g), fits the data better than fitted_model_2, which is from part (c).

However, we see that the p-value for the F test is 0.2677 which is greater than 0.05. This indicates that the result of extended_model_2 is not significant at the 5% significance level. So the additional terms are not significant enough to be included.
